{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ea7fd5f",
   "metadata": {},
   "source": [
    "# Tutoral  8: Missing Data Analysis\n",
    "## The Reality of Messy Data\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "By the end of this class, you will be able to:\n",
    "\n",
    "- **Systematically analyze missing data patterns** using both statistical and visual approaches\n",
    "- **Create missing data heatmaps** to identify patterns and correlations in missingness\n",
    "- **Apply strategic approaches** for handling missing values based on context and impact\n",
    "- **Integrate missing data analysis** into your exploratory data analysis workflow\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction: Why Missing Data Matters\n",
    "\n",
    "> *\"In the real world, data is never perfect. Learning to work with missing data isn't just a technical skill—it's essential for honest, robust analysis.\"*\n",
    "\n",
    "**Today's Focus:** While Tutorial 5 and 6 taught you advanced visualization techniques with clean movie data, today we'll apply these skills to messier, more realistic data. We'll also tackle one of data science's biggest challenges: missing data.\n",
    "\n",
    "**The Challenge:** How do we conduct meaningful exploratory data analysis when our data has gaps?\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd07c66c",
   "metadata": {},
   "source": [
    "### Hawks Dataset Data Card\n",
    "\n",
    "| Column       | Description   | Attribute Type |                                                               \n",
    "|--------------|---------------------------------------------------------|------|\n",
    "| month | Month of capture | Temporal |\n",
    "| day | date in the month | Temporal|\n",
    "| year | Year of capture | Temporal |\n",
    "| capturetime | time of capture (HH:MM) | Temporal |\n",
    "| releasetime | time of release (HH:MM) | Temporal |\n",
    "| bandnumber | ID band code | ... |\n",
    "| species      | hawk's species CH=Cooper's RT=Red-tailed, SS=Sharp-Shinned    | Nominal |\n",
    "| age          | age group: A=Adult or I=Immature | Ordinal  |\n",
    "| sex | f=female or M=Male |   Nominal |\n",
    "| wing         | Length (in mm) of primary wing feather from tip to wrist it attaches to | Quantitative |\n",
    "| weight       | Body weight (in gm)    | Quantitative |\n",
    "| culmen       | Length (in mm) of the upper bill from the tip to where it bumps into the fleshy part of the bird | Quantitative |\n",
    "| hallux       | Length (in mm) of the killing talon      | Quantitative |\n",
    "| tail         | Measurement (in mm) related to the length of the tail    | Quantitative         |\n",
    "| standardtail | standard measurement of tail length (in mm) | Quantitative |\n",
    "| tarsus | length of the basic foot bone (in mm) | Quantitative |\n",
    "| wingpitfat | amount of fat in the wing pit | Quantitative |\n",
    "| keelfat | amount of fat on the breastbone (measured by feel) |  ... |\n",
    "| crop | amount of material in the cro, coded from 1=full to 0 = empty | ... |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f86b158",
   "metadata": {},
   "source": [
    "## Dataset and Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa511863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "\n",
    "# FILEPATH\n",
    "# If on PL use this one\n",
    "filepath = 'data/hawks.csv'\n",
    "\n",
    "# If running locally on your machine use this one\n",
    "filepath = 'https://raw.githubusercontent.com/kemiolamudzengi/dsci-320-datasets/main/hawks.csv'\n",
    "\n",
    "# Load the hawks dataset\n",
    "hawks = pd.read_csv(filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2702c192",
   "metadata": {},
   "outputs": [],
   "source": [
    "hawks = (hawks.drop(columns=['Unnamed: 0'], errors='ignore').rename(columns=lambda x: x.strip().lower()))\n",
    "\n",
    "print(f\"Hawks dataset shape: {hawks.shape}\")\n",
    "print(f\"Columns: {list(hawks.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf5ea72",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Missing Data Landscape\n",
    "### *Understanding What's Missing and Why*\n",
    "\n",
    "Real data always has missing values. Before we can analyze patterns, we need to understand the missing data landscape. We will begin by using Pandas. \n",
    "\n",
    "### Pandas Approach\n",
    "We have done this before, but we will show once again how to calculate the number of missing items in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec69d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Overall missing data summary\n",
    "missing_summary = hawks.isnull().sum()\n",
    "print(\"Missing values per column:\")\n",
    "print(missing_summary[missing_summary > 0])\n",
    "\n",
    "\n",
    "# 2. Missing data percentages\n",
    "missing_percentages = (hawks.isnull().sum() / len(hawks) * 100).round(2)\n",
    "print(\"\\nPercentage missing per column:\")\n",
    "print(missing_percentages[missing_percentages > 0])\n",
    "\n",
    "\n",
    "# 3. Complete cases analysis\n",
    "complete_cases = hawks.dropna().shape[0]\n",
    "total_cases = hawks.shape[0]\n",
    "print(f\"\\nComplete cases: {complete_cases} out of {total_cases} ({complete_cases/total_cases*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc3c8f2",
   "metadata": {},
   "source": [
    "### Reflection: \n",
    "What do you notice from the tables?\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7978ed3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3af712",
   "metadata": {},
   "source": [
    "### Visualizing Missingness with Altair\n",
    "\n",
    "Remember that Altair is a declarative visualization grammar. \n",
    "One caveat is that it is not designed by default to handle missing values, so you need to prepare your data properly before visualizing.\n",
    "\n",
    "The code below does the following:\n",
    "\n",
    "* **Calculate missing percentages:** Determine the proportion of missing values for each column.\n",
    "* **Identify high-missing variables:** Flag columns with more than 30% missing data.\n",
    "* **Create summary table:** Include variable names, missing percentages, and category labels (high vs. acceptable missing).\n",
    "* **Visualize with Altair:** Plot a horizontal bar chart showing missing percentages, color-coded by category.\n",
    "* **Add threshold line:** Overlay a 30% line to highlight variables with high missingness.\n",
    "* **Purpose:** Quickly identify which variables have excessive missing data and may need special handling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cff9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate missing percentages for each column\n",
    "missing_percentages = hawks.isnull().mean()\n",
    "\n",
    "# Step 2: Identify high-missing variables (e.g., >30%)\n",
    "missing_threshold = 0.3\n",
    "high_missing_vars = missing_percentages[missing_percentages > missing_threshold].index.tolist()\n",
    "\n",
    "\n",
    "missing_data_summary = pd.DataFrame({\n",
    "        'variable': missing_percentages.index,\n",
    "        'missing_percentage': missing_percentages.values * 100,\n",
    "        'high_missing': [var in high_missing_vars for var in missing_percentages.index],\n",
    "        'variable_type': ['High Missing (>30%)' if var in high_missing_vars else 'Acceptable Missing' \n",
    "                         for var in missing_percentages.index]\n",
    "})\n",
    "missing_data_summary.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9abd72",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 5px solid #007BFF; padding: 1em; background-color: #F0F8FF;\">\n",
    "\n",
    "<h3><b>Viz Task: Missing Data Summary with Threshold Line</b></h3>\n",
    "\n",
    "<ul>\n",
    "<li>Create a layered bar chart showing the percentage of missing data for each variable.</li>\n",
    "\n",
    "<li>Use the <code>bar</code> mark for the main chart and a <code>rule</code> mark to indicate the threshold line.</li>\n",
    "\n",
    "<li>Mark and layer details:\n",
    "  <ul>\n",
    "    <li><b>Main Chart:</b>\n",
    "      <ul>\n",
    "        <li><code>mark_bar()</code> — creates bars showing missing percentages.</li>\n",
    "        <li>The width should be 300 and the height should be 350 — set chart dimensions.</li>\n",
    "        <li><code>title should be \"Missing Data by Variable\"</code> — adds a descriptive chart title.</li>\n",
    "      </ul>\n",
    "    </li>\n",
    "  </ul>\n",
    "</li>\n",
    "\n",
    "<li>Encode:\n",
    "  <ul>\n",
    "    <li><code>'missing_percentage'</code> — on the x channel</li>\n",
    "    <li><code>'variable'</code> on the y channel and sort by descending by x).</li>\n",
    "    <li><code>'variable_type'</code>  on the color channel\n",
    "      <ul>\n",
    "        <li><code>scale=alt.Scale(range=['steelblue', 'orange'])</code> — sets color palette.</li>\n",
    "        <li><code>legend=alt.Legend(title=\"Missing Data Level\")</code> — adds a legend.</li>\n",
    "      </ul>\n",
    "    </li>\n",
    "    </ul>\n",
    "</li>\n",
    "\n",
    "<li>Layer the bar chart and threshold line using <code>alt.layer()</code> and display with <code>missing_with_threshold</code>.</li>\n",
    "\n",
    "</ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ce8e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_chart = alt.Chart(missing_data_summary).mark_bar().encode(\n",
    "    x=alt.X('missing_percentage:Q', title='Missing Data %'),\n",
    "    y=alt.Y('variable:N', sort='-x', title='Variable'),\n",
    "    color=alt.Color('variable_type:N',\n",
    "                    scale=alt.Scale(range=['steelblue', 'orange']),\n",
    "                    legend=alt.Legend(title=\"Missing Data Level\")),\n",
    "    tooltip=['variable:N', alt.Tooltip('missing_percentage:Q', format='.1f')]\n",
    ").properties(\n",
    "    width=300,\n",
    "    height=350,\n",
    "    title=\"Missing Data by Variable\"\n",
    ")\n",
    "\n",
    "# Add threshold line\n",
    "threshold_line = alt.Chart(pd.DataFrame({'threshold': [missing_threshold * 100]})).mark_rule(\n",
    "    color='red', strokeDash=[5, 5], size=2\n",
    ").encode(\n",
    "    x='threshold:Q'\n",
    ")\n",
    "\n",
    "missing_with_threshold = alt.layer(missing_chart, threshold_line)\n",
    "\n",
    "# Show\n",
    "missing_with_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75c0649",
   "metadata": {},
   "source": [
    "## Visual Approach: Introducing MissingNo\n",
    "\n",
    "Now this is a visualization course, so we will try to highlight viz options for missing data. There are other libraries that are more effectively at focusing on visualization missing data. Remember that Altair is primarily for viz, but others libraries have focused on data wrangling and so instead of reeventing the wheel we will use those libraries as needed. \n",
    "\n",
    "[MissingNo Library](https://github.com/ResidentMario/missingno)\n",
    "\n",
    "So let's use it to get a sense of the missingness in our dataset. <br>\n",
    "First install the library <br>\n",
    "`pip install missingno`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cc2c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncommment the command below and run this cell\n",
    "\n",
    "#!pip install missingno\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f2f7ba",
   "metadata": {},
   "source": [
    "Now that you have installed the missningno library, let's import it and call the functions on our dataset to get a better sense of what is going on?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955cff4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "msno.bar(hawks)             # percent-missing bars\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f53b37e",
   "metadata": {},
   "source": [
    "So this mirrors the data from what we had above when we used pandas. \n",
    "\n",
    "##### Interpreting the Visualization\n",
    "Let's deconstruct the viz to ensure we understand what information it presents. \n",
    "\n",
    " - The left y-channel our axis shows the proportion of data that is present. \n",
    " - While the right y-channel our axis shows the number of available values.  \n",
    " - Notice that the height of the bar can be interpreted by using either axes values. So you can either say the sex has ~35% of values, or that the sex column has 340ish values. \n",
    " - On top of each bar we have the number of available values for each attribute. \n",
    "\n",
    "<br>\n",
    "What we don't yet have a sense of, is what is the relationship between the missing values? For instance, is it possible that records missing the tarsus value are also missing wingpitfat?\n",
    "\n",
    "Let's investigate. Run the cell below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc39deeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(hawks)          # nullity matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607a24a5",
   "metadata": {},
   "source": [
    "#### Interpreting the viz. \n",
    "This is a composite visualization that includes a matrix and a sparkline. \n",
    "\n",
    "##### Matrix\n",
    "- The y-channel encodes the record number\n",
    "- The x-channel encodes the attribute.\n",
    "- The color-channel encodes missingness for the given attribute and record. White lines (or blank spaces) indicate missing (null) values. While gray lines (or colored) indicate non-missing values.\n",
    "\n",
    "With this matrix we are able to get a sense of the co-location of missing values across all records. \n",
    "\n",
    "##### Sparkline\n",
    "The sparkline gives you a quick sense of how complete each row is.\n",
    "If the sparkline is mostly flat and near the top, most rows have few missing values. If it varies a lot, some rows are missing much more data than others. The text 19 that you see on the sparkline shows the (minimum nullity) which is the number of values for the most complete row, while 11 indicates the (maximum nullity), which is the number of values for the row with the most missing values. \n",
    "\n",
    "<div style=\"background-color: #aff1cd;\">\n",
    "    <h3>Discussion Questions </h3>\n",
    "\n",
    "What else can you learn from this visualization\n",
    "1. Which variables have the most missing data?\n",
    "2. Are missing patterns random, or do they seem related to species/sex?\n",
    "3. What might cause these missing data patterns in a real bird study?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b5bac7",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad430d56",
   "metadata": {},
   "source": [
    "### Missing Heatmap\n",
    "We previously had the question, are the missing patterns random? While the matrix should co-occurence of missingness it doesn't show correlation, the library has another viz that supports answering this question.\n",
    "\n",
    "The missingno heatmap shows the correlation where depicts how the presence or absence of one variable is related to another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56a812a",
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.heatmap(hawks)         # missingness correlation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd978790",
   "metadata": {},
   "source": [
    "#### Heatmap Interpretation\n",
    "\n",
    "* **Correlation scale:**\n",
    "\n",
    "  * **-1:** Perfect *negative* nullity correlation → if one variable is present, the other is always missing.\n",
    "  * **0:** *No* relationship → missingness in one variable has no effect on the other.\n",
    "  * **+1:** Perfect *positive* nullity correlation → if one variable is missing (or present), the other is too.\n",
    "\n",
    "* **Automatically excluded columns:**\n",
    "  Columns that are **completely full** or **completely empty** (no variation in nullity) are removed, since their correlation is meaningless.\n",
    "\n",
    "* **Interpreting near-perfect correlations (<1 or >-1):**\n",
    "  Indicates *almost perfect* relationships, but small deviations may reveal **erroneous or inconsistent records** in the dataset.\n",
    "\n",
    "* **Best use case:**\n",
    "\n",
    "  * Detecting **pairs of variables** that share similar missing data patterns.\n",
    "  * Helpful for identifying **dependencies** or **shared causes** of missingness.\n",
    "\n",
    "* **Limitations:**\n",
    "\n",
    "  * Doesn’t easily capture **larger or multivariate** relationships.\n",
    "  * Not ideal for **very large datasets**, where it becomes harder to interpret.\n",
    "\n",
    "##### Use case\n",
    "So now what do you notice about hallux & culmen? Or keelfat & crop?\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ef61f3",
   "metadata": {},
   "source": [
    "### UpsetPlot\n",
    "Let's look at one more visualization that provides a detailed visualization of missingness. \n",
    "UpSet plot/diagram is a visualization that shows **which combinations of columns** have missing values **together**, and **how many rows** fall into each pattern. It’s an alternative to Venn diagrams — much more effective when you have **many variables**.\n",
    "Let's use it by first installing the library and then calling the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58ac078",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment command below to install\n",
    "\n",
    "#!pip install upsetplot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29868a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from upsetplot import from_indicators, UpSet\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "\n",
    "# Create a missingness indicator DataFrame\n",
    "missing_indicators = hawks.isna()\n",
    "\n",
    "# Convert to the format upsetplot expects\n",
    "upset_data = from_indicators(missing_indicators.columns, missing_indicators)\n",
    "\n",
    "# Plot\n",
    "UpSet(upset_data, subset_size='count', show_counts=True, sort_by='degree').plot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7cc632",
   "metadata": {},
   "source": [
    "#### UpSet Interpretation\n",
    "The **bottom** shows dots connected by lines — these represent **combinations** of missingness across columns.\n",
    "- A **single dot** → rows missing in only that one column.\n",
    "- **Connected dots** → rows missing in multiple columns at once. <br>\n",
    "The **top bar chart** shows how many rows fall into each missingness combination. <br>\n",
    "The **left bar chart** shows the **total number of missing values per column** (like a summary).\n",
    "\n",
    "**What it helps you see:**\n",
    "  * Which variables tend to be **missing together**.\n",
    "  * Whether missingness is **isolated to certain columns** or **co-occurs across several**.\n",
    "  * Which combinations are most common (e.g., “these 3 columns are often missing in the same rows”).\n",
    "\n",
    "**Why it’s useful:**\n",
    "  * More scalable than a Venn diagram (which gets messy after 3–4 sets).\n",
    "  * Helps detect **structured missingness** — e.g., groups of variables that fail together.\n",
    "\n",
    "    \n",
    "##### Upset Use Case\n",
    "How many records have both the tarsus & sex attributes missing?\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f23e49",
   "metadata": {},
   "source": [
    "## Missingness by a Categorical Attribute\n",
    "So far we have focused on overall missingness, but what if we wanted to focus in on specific nominal attributes, what would we do. Previously, we have used pandas and so let's revisit our numerical approach before presenting the visual options. \n",
    "\n",
    "### Pandas Approach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ddab63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are missing patterns related to species?\n",
    "species_missing = hawks.groupby('species').apply(lambda x: x.isnull().sum())\n",
    "print(\"Missing values by species:\")\n",
    "print(species_missing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87948fdf",
   "metadata": {},
   "source": [
    "### Visual Approach\n",
    "<div style=\"border-left: 5px solid #007BFF; padding: 1em; background-color: #F0F8FF;\">\n",
    "\n",
    "<h3><b>Viz Task: Counts of Hawks by Species and Missing Wing Pit Fat</b></h3>\n",
    "\n",
    "<ul>\n",
    "<li>Create a <code>bar</code> chart to show how many hawks belong to each species and whether their <code>wingpitfat</code> value is missing.</li>\n",
    "\n",
    "<li>Encode:\n",
    "  <ul>\n",
    "    <li><code>species</code> on the <b>y channel</b> as nominal.</li>\n",
    "    <li><code>count()</code> on the <b>x channel</b> to show the number of records per species.</li>\n",
    "    <li><code>wingpitfat_missing</code> on the <b>color channel</b> to distinguish missing versus non-missing cases.</li>\n",
    "  </ul>\n",
    "</li>\n",
    "\n",
    "<li>Add tooltips to display <code>species</code>, <code>wingpitfat_missing</code>, and <code>count()</code> when hovering over bars.</li>\n",
    "\n",
    "<li>Set the chart title to <b>\"Count of Hawks by Species and Wing Pit Fat Missing Status\"</b>.</li>\n",
    "</ul>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8be992f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hawks[\"wingpitfat_missing\"] = hawks[\"wingpitfat\"].isna()\n",
    "\n",
    "alt.Chart(hawks).mark_bar().encode(\n",
    "    y=alt.Y('species:N', title='Species'),\n",
    "    x=alt.X('count():Q', title='Count'),\n",
    "    color=alt.Color('wingpitfat_missing:N', title='Is Wing Pit Fat Missing?'),\n",
    "    tooltip=['species', 'wingpitfat_missing', 'count()']\n",
    ").properties(\n",
    "    title='Count of Hawks by Species and Wing Pit Fat Missing Status'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf4343d",
   "metadata": {},
   "source": [
    "Now why is this not the best way to visualize this data? \n",
    "<br>\n",
    "...\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "What errorneous conclusions can you draw from this current visualization?\n",
    "\n",
    "<br>\n",
    "---\n",
    "\n",
    "\n",
    "Remember from our density and histogram plots there are significantly more Red Tail hawks that were tagged and observed than Coopers or Sharp Shinned. So just showing the count makes it hard to get a sense of what is happening for each specie.  A better approach would be to normalize the bar charts to get a sense of the proportion of missing values for each specie. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34d0cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(hawks).mark_bar().encode(\n",
    "    y=alt.Y('species:N', title='Species'),\n",
    "    x=alt.X('count():Q', title='Count').stack(\"normalize\"),\n",
    "    color=alt.Color('wingpitfat_missing:N', title='Is Wing Pit Fat Missing?'),\n",
    "    tooltip=['species', 'wingpitfat_missing', 'count()']\n",
    ").properties(\n",
    "    title=\"Proportion of Missing Wingpitfat Values by Species\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43523dd0",
   "metadata": {},
   "source": [
    "We can do this for all the quantitative attributes that have a high missingness threshold to get a sense of how excluding rows may impact our dataset. \n",
    "I'm getting ahead of myself. \n",
    "So how are we going to deal with all this missingness. \n",
    "\n",
    "Let's start by laying out the possibly options and then execute each one and then evaluate it. While we call it data science, this part of it, the data wrangling aspect is a bit subjective, there is no engineered best process, there are just a few different processes each with its limitations. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7eb313e",
   "metadata": {},
   "source": [
    "## Dealing with Missing Values\n",
    "### Strategic Decision Making\n",
    "\n",
    "Not all missing data strategies are created equal. The right approach depends on your analysis goals and the missing data mechanism.\n",
    "\n",
    "### Strategy 1: Agressive Row Deletion Approach\n",
    "Remove all rows with ANY missing valuel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c89c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before: Original dataset\n",
    "print(f\"Original dataset: {hawks.shape}\")\n",
    "\n",
    "# After: Remove all rows with at least one missing value. Keep only complete cases\n",
    "hawks_complete = hawks.dropna()\n",
    "print(f\"Complete cases only: {hawks_complete.shape}\")\n",
    "# Calculate total values (rows × columns)\n",
    "original_total_values = hawks.shape[0] * hawks.shape[1]\n",
    "retained_total_values = hawks_complete.shape[0] * hawks_complete.shape[1]\n",
    "\n",
    "print(f\"Data retention: {retained_total_values/original_total_values*100:.1f}%\")\n",
    "print(f\"Data loss: {(1 - retained_total_values/original_total_values)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03311a6b",
   "metadata": {},
   "source": [
    "If we were to remove all the rows that had at least one missing value, we would lose 96% of our data. That is **HUGE!!!. **\n",
    "So maybe this isn't the best idea. \n",
    "\n",
    "\n",
    "### Strategy 2:  Tiered Filtering (Column-Then-Row Deletion)\n",
    "First drop all high-missing columns (i.e., columns with missing values over x (where x is a value >=50%)) and then remove incomplete rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0f219d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify variables with high missingness\n",
    "high_missing = missing_percentages[missing_percentages > 30]\n",
    "print(\"Attributes that have more than 50% of values missing:\")\n",
    "print(high_missing)\n",
    "\n",
    "# Strategic approach: Drop high-missing variables, keep others\n",
    "hawks_strategic = hawks.drop(columns=high_missing.index) # drop columns with missing data >50%\n",
    "hawks_strategic_complete = hawks_strategic.dropna() # drop the remaining rows with missing data \n",
    "\n",
    "print(f\"After strategic dropping: {hawks_strategic_complete.shape}\")\n",
    "\n",
    "# Calculate total values (rows × columns)\n",
    "original_total_values = hawks.shape[0] * hawks.shape[1]\n",
    "retained_total_values = hawks_strategic_complete.shape[0] * hawks_strategic_complete.shape[1]\n",
    "\n",
    "print(f\"Data retention: {retained_total_values/original_total_values*100:.1f}%\")\n",
    "print(f\"Data loss: {(1 - retained_total_values/original_total_values)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8507a64",
   "metadata": {},
   "source": [
    "In the second approach, we retain 69% of our data. This is significantly better than the previous approach, which kept only 4% of the data. While raw percentages are one way to assess the impact of our decisions, a vital step is to explore how these strategies affect the distribution of our attributes.\n",
    "<br>\n",
    "Two key considerations:\n",
    "- What is the impact on the quantitative attributes (e.g., wing length, weight)?\n",
    "- What is the impact on the distribution across different groups (i.e., nominal attributes like species and sex)?\n",
    "\n",
    "\n",
    "### **Impact Visualization: Before vs After**\n",
    "Below we implement a function that we can re-use to show the distribution for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da41cc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layered_density_chart(data, title):\n",
    "    \"\"\"Create a layered density chart where color represents species.\"\"\"\n",
    "    return (\n",
    "        alt.Chart(data)\n",
    "        .transform_density(\n",
    "            'wing',\n",
    "            groupby=['species'],\n",
    "            as_=['wing', 'density']\n",
    "        )\n",
    "        .mark_area(opacity=0.7)\n",
    "        .encode(\n",
    "            x=alt.X('wing:Q', title='Wing Length (mm)'),\n",
    "            y=alt.Y('density:Q', title='Density'),\n",
    "            color=alt.Color('species:N', title='Species')\n",
    "        )\n",
    "        .properties(title=title, width = 200, height=220)\n",
    "    )\n",
    "\n",
    "# Apply function to each dataset\n",
    "base = layered_density_chart(hawks, 'Original Dataset')\n",
    "complete = layered_density_chart(hawks_complete, 'Aggressive Row Deletion')\n",
    "strategic = layered_density_chart(hawks_strategic_complete, 'Tiered Filtering')\n",
    "\n",
    "# Combine into columns\n",
    "(base | complete | strategic).resolve_scale(y='independent')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f768dfa",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "These density plots show how different approaches to handling missing data affect wing-length distributions by species (`CH`, `RT`, `SS`).\n",
    "\n",
    "* **Original Dataset:** Clear, distinct species distributions — the baseline representation of natural variation.\n",
    "* **Aggressive Row Deletion:** Dropping all rows with missing values distorts the picture. Some species are underrepresented, and the overall spread shrinks, indicating biased data loss.\n",
    "* **Tiered Filtering:** A selective approach that retains most of the original structure while still addressing missingness.\n",
    "\n",
    "It’s essential to consider **nominal attributes** when cleaning data: if the distribution of a nominal variable changes, the dataset may no longer represent the true population.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75cb607",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #aff1cd;\">\n",
    "    <h3>Discussion Questions</h3>\n",
    "    Play around with the threshold percentage we used in Strategy 2.\n",
    "    We said if half of the values are missing drop those columns, and then we removed any rows with mising values that remained. \n",
    "    What happens if we decrease the mising threshold to 40%, 30%, 20%, What impact does this have on the number of columns we keep and the distribution for various attributes such as (wing length, weight). \n",
    "   Can you think of one disadvantage of dropping columns just based on threshold? \n",
    "    <br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff14aa2",
   "metadata": {},
   "source": [
    "### Correlation Impact on Missingness Removal\n",
    "We removed columns from the dataset that may have had important information. When dealing with missing \n",
    "data, it’s often better to reduce redundancy before deciding what to impute or drop. \n",
    "Highly correlated variables carry overlapping information, so keeping both \n",
    "can exaggerate the impact of missing values and add noise. \n",
    "By first identifying and retaining only the more complete variable in each correlated pair, \n",
    "this approach simplifies the dataset and minimizes information loss while improving data \n",
    "quality for downstream analysis.\n",
    "\n",
    "To get started we are first going to visualize the correlation between all quantitative attributes. Instead of using Altair, I will introduce another viz. library, [Seaborn](https://seaborn.pydata.org/) that has in-built functions that do the heavy data wrangling we typically do (from wide to long) for us. \n",
    "\n",
    "uncomment and run the cell below to install seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63788631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT COMMAND BELOW TO INSTALL \n",
    "#!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f7d66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "corr = hawks.corr(numeric_only=True)\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    " \n",
    "\n",
    "sns.heatmap(\n",
    "    corr, \n",
    "    cmap=\"RdBu\",  # diverging palette\n",
    "    vmin=-1, vmax=1,\n",
    "    center=0,\n",
    "    annot=False,# fmt=\".2f\",\n",
    "    square=True,\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={\"shrink\": 0.8}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce86e20a",
   "metadata": {},
   "source": [
    "### **What the Code Does**\n",
    "\n",
    "This code creates a **correlation heatmap** using Seaborn, showing pairwise correlations between numeric variables.\n",
    "\n",
    "* `corr`: your correlation matrix (values between −1 and 1).\n",
    "* `cmap=\"RdBu\"`: a *diverging* red–blue colormap.\n",
    "* `vmin=-1, vmax=1, center=0`: ensures the scale runs symmetrically from −1 (strong negative) to +1 (strong positive).\n",
    "* `annot=False`: hides the correlation values on the cells (set to `True` if you want numbers).\n",
    "* `square=True`: makes each cell square.\n",
    "* `linewidths=0.5`: adds thin grid lines between cells.\n",
    "* `cbar_kws={\"shrink\": 0.8}`: makes the color bar smaller.\n",
    "\n",
    "\n",
    "<div style=\"background-color: #aff1cd;\">\n",
    "    <h3>Discussion Question</h3>\n",
    "\n",
    "Now that we have a sense of the correlations between attributes, we are going to analyze which attributes are strongly correlated and consider removing the one with more missing values. I could write out the code quickly, but why might this be a bad idea? \n",
    "\n",
    "<br>Why is it risky to remove attributes just because they are strongly correlated with others? \n",
    "\n",
    "<br>When should we actually remove such attributes? \n",
    "    <br>Should we always remove them, or only when they have a high level of missingness? </div>\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e794bb6",
   "metadata": {},
   "source": [
    "\n",
    "### Strategy 3: Correlation-Aware Redundancy Removal\n",
    "\n",
    "I am going to present another strategy: correlation-aware redundancy removal. It is similar to the tiered approach, but first we determine the correlation between attributes. Then, for attributes that are highly correlated to each other AND have high missingness, we remove them. After this, we remove the rows with missing values.\n",
    "\n",
    "#### **Step-by-Step Explanation of the Correlation-Aware Redundancy Removal Function**\n",
    "\n",
    "1. **Identify variables with lots of missing data.**\n",
    "   The code computes the percentage of missing values for each column and flags any variable exceeding the `missing_threshold` (e.g., 30%) as a candidate for possible removal.\n",
    "\n",
    "2. **Compute correlations among numeric variables.**\n",
    "   It creates a correlation matrix of all numeric columns and takes the absolute values to measure the strength (not direction) of relationships.\n",
    "\n",
    "3. **Focus only on high-missing variables.**\n",
    "   For each variable with high missingness, it checks how strongly it correlates (above `correlation_threshold`, e.g., 0.9) with other numeric variables.\n",
    "\n",
    "4. **Decide which variable to remove.**\n",
    "\n",
    "   * If the correlated variable has **fewer missing values**, remove the high-missing one.\n",
    "   * If **both** are highly missing, remove whichever has the **greater missing percentage**.\n",
    "\n",
    "5. **Output the removal list.**\n",
    "   It compiles all identified variables (deduplicated) and returns them as the recommended attributes to drop.\n",
    "\n",
    "<div style=\"border-left: 5px solid #1E7D32; padding: 1em; background-color: #4CAF50; color: white;\">\n",
    "<h3><b>Important: Learning Focus Disclaimer</b></h3>\n",
    "You are not required to understand the minutae of the function below.\n",
    "Instead focus on the high-level process we have presented above.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca011960",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strategic_redundant_removal(df, missing_threshold=0.3, correlation_threshold=0.9):\n",
    "    \"\"\"\n",
    "    Remove redundant variables, but only if they have high missing data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Identify high-missing variables (candidates for removal)\n",
    "    missing_percentages = df.isnull().mean()\n",
    "    high_missing_vars = missing_percentages[missing_percentages > missing_threshold].index.tolist()\n",
    "    \n",
    "    print(f\"Variables with >{missing_threshold*100}% missing data (candidates for removal):\")\n",
    "    for var in high_missing_vars:\n",
    "        print(f\"  {var}: {missing_percentages[var]*100:.1f}% missing\")\n",
    "    \n",
    "    # Step 2: Find correlations only involving high-missing variables\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    correlation_matrix = df[numeric_cols].corr().abs()\n",
    "    \n",
    "    removal_candidates = []\n",
    "    \n",
    "    # Check correlations between high-missing vars and all other vars\n",
    "    for high_missing_var in high_missing_vars:\n",
    "        if high_missing_var in correlation_matrix.columns:\n",
    "            correlations = correlation_matrix[high_missing_var]\n",
    "            \n",
    "            # Find highly correlated variables\n",
    "            high_corr_vars = correlations[correlations > correlation_threshold].index.tolist()\n",
    "            high_corr_vars.remove(high_missing_var)  # Remove self-correlation\n",
    "            \n",
    "            for corr_var in high_corr_vars:\n",
    "                corr_value = correlations[corr_var]\n",
    "                missing_high = missing_percentages[high_missing_var]\n",
    "                missing_corr = missing_percentages[corr_var]\n",
    "                \n",
    "                print(f\"\\nHigh correlation found:\")\n",
    "                print(f\"  {high_missing_var} ({missing_high*100:.1f}% missing) vs\")\n",
    "                print(f\"  {corr_var} ({missing_corr*100:.1f}% missing)\")\n",
    "                print(f\"  Correlation: {corr_value:.3f}\")\n",
    "                \n",
    "                # Decision logic\n",
    "                if missing_corr <= missing_threshold:\n",
    "                    # The correlated variable has acceptable missing data - remove the high-missing one\n",
    "                    print(f\"  → Remove {high_missing_var} (keep {corr_var})\")\n",
    "                    removal_candidates.append(high_missing_var)\n",
    "                else:\n",
    "                    # Both have high missing data - remove the worse one\n",
    "                    if missing_high >= missing_corr:\n",
    "                        print(f\"  → Remove {high_missing_var} (worse missing data)\")\n",
    "                        removal_candidates.append(high_missing_var)\n",
    "                    else:\n",
    "                        print(f\"  → Remove {corr_var} (worse missing data)\")\n",
    "                        removal_candidates.append(corr_var)\n",
    "    \n",
    "    # Remove duplicates and actually drop the columns\n",
    "    removal_candidates = list(set(removal_candidates))\n",
    "    \n",
    "    print(f\"\\nCandidate Attributes recommended for removal: {removal_candidates}\")\n",
    "    \n",
    "    return removal_candidates\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dd1fb7",
   "metadata": {},
   "source": [
    "##### Calling the Function\n",
    "\n",
    "Now let's call the function we created and see how different thresholds may impact which attributes get removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1047aed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_vars = strategic_redundant_removal(hawks, missing_threshold=0.3, correlation_threshold=0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dae1bda",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "When we use a missing threshold of 30% and a correlation threshold of 85%, the `tarsus` and `standardtail` attributes — which are highly correlated with `hallux` and `tail` — are recommended for removal. So instead of just going by missingness threshold, we focus on highly correlated columns AND missingness threshold. This new approach is less agressive than an earlier approach which had us removing `standardtail`, `tarsus`, AND `wingpitfat`, `keelfat`, `sex`, and `crop`. \n",
    "\n",
    "Notice that `standardtail` is highly correlated with `wing`, `weight` and `tail`. So dropping this attribute doesn't had a profound impact on our understanding of the data. However, notice how `wingpitfat` which has a 92% missingness is not highly correlated with any of the more complete attributes. It has a negative correlation of around 0.4. So what do we do now?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd1b26e",
   "metadata": {},
   "source": [
    "So we have explored different ways to determine which columns to keep. \n",
    "In general, you can use the following as a guide.\n",
    "\n",
    "**Strategy Selection Framework:**\n",
    "- **< 5% missing**: Usually safe to use complete cases\n",
    "- **5-15% missing**: Consider impact on sample size and bias\n",
    "- **> 15% missing**: Investigate patterns; consider imputation or flagging\n",
    "- **> 40% missing**: Usually drop the variable unless critical\n",
    "\n",
    "---\n",
    "So unfortunately `wingpitfat` is going to have to disappear. With a loss of 90% it is almost impossible to redeem. For `sex` we will keep becaue it is a nominal attribute, we just need to always remember for our analysis that when we use this variable we are excluding over 60% of records. \n",
    "\n",
    "For `standardtail` let's consider an alternative approach, instead of dropping the column, how about we use imputation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4deacae",
   "metadata": {},
   "source": [
    "## Imputation\n",
    "\n",
    "Imputation is the process of replacing `NA` values with estimated values determined using various methods. Note that you are **not required to implement imputation** in this course, but you should be aware of the different approaches, as well as their benefits and limitations. The four methods presented here are examples to help you become familiar with common strategies.\n",
    "\n",
    "\n",
    "### **1. Simple Statistical Imputation**\n",
    "\n",
    "Simple statistical imputation replaces missing values with a summary statistic (mean, median, or mode) calculated from the available data. This is the most straightforward imputation approach and serves as a baseline method.\n",
    "\n",
    "**How it works:**\n",
    "- Calculate a central tendency measure (mean, median, mode) from non-missing values\n",
    "- Replace all missing values with this single statistic\n",
    "- For quantitative data: typically use mean or median\n",
    "- For categorical data: use mode (most frequent value)\n",
    "\n",
    "**When to use:**\n",
    "- Quick exploratory analysis\n",
    "- Less than 10% missing data\n",
    "- When relationships between variables are not critical\n",
    "- Time-constrained situations\n",
    "\n",
    "---\n",
    "\n",
    "#### **Implementation with Hawks Dataset**\n",
    "\n",
    "```python\n",
    "\n",
    "# Option 1: Median(most robust, especically in addressing outliers.)\n",
    "hawks['standardtail'] = hawks['standardtail'].fillna(hawks['standardtail'].median())\n",
    "\n",
    "# Option 2: Mean imputation (if distribution is roughly normal)\n",
    "hawks['standardtail'] = hawks['standardtail'].fillna(hawks['standardtail'].mean())\n",
    "\n",
    "# Option 3: Mode imputation (if keelfat has discrete values)\n",
    "hawks['standardtail'] = hawks['standardtail'].fillna(hawks['standardtail'].mode()[0])\n",
    "\n",
    "# Option 4: Forward fill (if there's temporal ordering)\n",
    "hawks['standardtail'] = hawks['standardtail'].fillna(method='ffill')\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Pros and Cons**\n",
    "\n",
    "Pros:\n",
    "- **Simplicity**: Easy to implement and understand\n",
    "- **Speed**: Computationally efficient, works on any size dataset\n",
    "- **No additional information needed**: Only requires the variable itself\n",
    "- **Preserves sample size**: Retains all observations for analysis\n",
    "- **Transparency**: Easy to explain to non-technical stakeholders\n",
    "- **Reproducible**: Same result every time with fixed data\n",
    "\n",
    "Cons:\n",
    "- **Reduces variance**: Makes distribution more concentrated around center\n",
    "- **Ignores relationships**: Doesn't use correlations between variables\n",
    "- **Distorts distributions**: Can create artificial peaks at imputed values\n",
    "- **Underestimates uncertainty**: Treats imputed values as known\n",
    "- **Biased with non-random missingness**: If missingness relates to the value itself\n",
    "- **Same value for all missing**: Doesn't reflect natural variability\n",
    "\n",
    "Watch out for:\n",
    "- Creating artificial concentration of values (visible in histogram)\n",
    "- Using with high percentages of missing data (>20%)\n",
    "- Combining with correlation analysis (artificially reduces correlations)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5567b118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layered_density_chart(data, title):\n",
    "    \"\"\"Create a layered density chart where color represents species.\"\"\"\n",
    "    return (\n",
    "        alt.Chart(data)\n",
    "        .transform_density(\n",
    "            'standardtail',  \n",
    "            groupby=['species'],\n",
    "            as_=['standardtail', 'density']  \n",
    "        )\n",
    "        .mark_area(opacity=0.7)\n",
    "        .encode(\n",
    "            x=alt.X('standardtail:Q', title='standardtail (mm)'),  \n",
    "            y=alt.Y('density:Q', title='Density'),\n",
    "            color=alt.Color('species:N', title='Species')\n",
    "        )\n",
    "        .properties(title=title, width=200, height=220)\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# Create different imputation versions\n",
    "hawks_mean = hawks.copy()\n",
    "hawks_mean['standardtail'] = hawks_mean['standardtail'].fillna(hawks_mean['standardtail'].mean())\n",
    "\n",
    "hawks_median = hawks.copy()\n",
    "hawks_median['standardtail'] = hawks_median['standardtail'].fillna(hawks_median['standardtail'].median())\n",
    "\n",
    "hawks_mode = hawks.copy()\n",
    "hawks_mode['standardtail'] = hawks_mode['standardtail'].fillna(hawks_mode['standardtail'].mode()[0])\n",
    "                                                         \n",
    "# Apply function to each dataset\n",
    "original = layered_density_chart(hawks, 'Original (with Missing)')\n",
    "mean_imp = layered_density_chart(hawks_mean, 'Mean Imputation')\n",
    "median_imp = layered_density_chart(hawks_median, 'Median Imputation')\n",
    "mode_imp = layered_density_chart(hawks_mode, 'Mode Imputation')\n",
    "\n",
    "# Combine into columns\n",
    "(original | mean_imp | median_imp | mode_imp).resolve_scale(y='independent')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83afe7ff",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "The **original (with missing)** chart shows natural variation — the red, orange, and blue distributions overlap but maintain distinct peaks. Once you start imputing missing values, the distributions become progressively **narrower and more peaked**, especially for median and mode imputation.\n",
    "This is because each imputation method introduces different assumptions about what “missing” means.\n",
    "\n",
    "#####  **Mean Imputation**\n",
    "\n",
    "* Each missing value is replaced with the **mean** for that species.\n",
    "* This pulls the distribution **toward the center** and slightly increases the peak.\n",
    "* Variance is reduced (especially visible in the orange and blue groups).\n",
    "* Still retains some natural shape because the mean doesn’t distort everything equally.\n",
    "\n",
    "##### **Median Imputation**\n",
    "\n",
    "* Replacing with the **median** creates an even sharper spike.\n",
    "* The distribution becomes **artificially narrow**, centered at the median.\n",
    "* This heavily underestimates the variability — everything clusters at the central value.\n",
    "* It looks unnaturally “tall” because the same value (the median) repeats often. Notice the different values on the y-axes. \n",
    "\n",
    "##### **Mode Imputation**\n",
    "\n",
    "* Here, missing values are replaced with the **most frequent (mode)** value.\n",
    "* That causes an **extreme spike** (almost vertical line), as seen in the far-right chart.\n",
    "* The distribution loses nearly all variability.\n",
    "* It’s a severe distortion — no longer represents the true shape of the data.\n",
    "* It can make downstream models (e.g., regressions) behave as if there’s a “perfect” cluster.\n",
    "\n",
    "\n",
    "####  Interpretation\n",
    "\n",
    "Imputation can make your data look “clean,” but it **distorts the distribution** — particularly the variance and tails.\n",
    "For analyses sensitive to distribution shape (like correlations, regression assumptions, or outlier detection), this distortion can **bias results**. We intention showed the different species, go back to the visualizations and look at how the distribution is different for all of the species. Why is this problematic?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811f014f",
   "metadata": {},
   "source": [
    "### **2. Group-Based (Conditional) Imputation**\n",
    "\n",
    "Group-based imputation recognizes that observations naturally cluster into meaningful groups, and missing values should be imputed using statistics from their specific group rather than the overall population. This approach is more sophisticated than simple imputation and often more accurate.\n",
    "\n",
    "**How it works:**\n",
    "- Identify relevant categorical grouping variables (e.g., species, sex, age)\n",
    "- Calculate statistics (mean, median) within each group\n",
    "- Replace missing values with the statistic from their respective group\n",
    "- Falls back to overall statistic if group has no non-missing values\n",
    "\n",
    "**When to use:**\n",
    "- Clear categorical groupings exist in the data\n",
    "- Groups have different distributions (e.g., different species have different average sizes)\n",
    "- Sufficient data within each group (at least 5-10 observations)\n",
    "- More accuracy needed than simple imputation can provide\n",
    "\n",
    "---\n",
    "\n",
    "#### **Implementation with Hawks Dataset**\n",
    "\n",
    "```python\n",
    "# Impute wing length using species-specific median\n",
    "hawks['standardtail'] = hawks.groupby('species')['standardtail'].transform(\n",
    "    lambda x: x.fillna(x.median())\n",
    ")\n",
    "# Multi-group imputation using mean\n",
    "hawks['standardtail'] = hawks.groupby(['species', 'sex'])['standardtail'].transform(\n",
    "    lambda x: x.fillna(x.mean())\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### **Pros and Cons**\n",
    "\n",
    "Pros:\n",
    "- **More accurate**: Respects natural variation between groups\n",
    "- **Preserves group differences**: Maintains distinct distributions across categories\n",
    "- **Reduces bias**: Better than simple imputation when groups truly differ\n",
    "- **Intuitive**: Makes logical sense (e.g., male hawks ≠ female hawks)\n",
    "- **Flexible**: Can use multiple grouping variables hierarchically\n",
    "- **Still simple**: Easy to implement and explain\n",
    "\n",
    "Cons:\n",
    "- **Requires categorical variables**: Need meaningful grouping variables\n",
    "- **Needs sufficient group sizes**: Small groups problematic (< 5-10 obs)\n",
    "- **Still reduces variance**: Within-group variance still compressed\n",
    "- **Doesn't capture cross-group relationships**: Ignores correlations with other continuous variables\n",
    "- **Can introduce group-level bias**: If missingness related to group membership\n",
    "- **Multiple comparisons**: More groups = more opportunities for odd results\n",
    "\n",
    "Watch out for:\n",
    "- Groups with very few observations (unstable estimates)\n",
    "- Empty groups (requires fallback strategy)\n",
    "- Overlapping group definitions\n",
    "- Too many groups (over-segmentation)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1c5a78",
   "metadata": {},
   "source": [
    "### **3. Interpolation Methods**\n",
    "\n",
    "Interpolation estimates missing values by assuming a smooth relationship between neighboring data points, making it ideal for **ordered data** like time series or spatial measurements.\n",
    "\n",
    "**How it works:** Fits a curve (linear, polynomial, or spline) through known points and estimates missing values along that curve.\n",
    "\n",
    "**When to use:** Time series, sensor readings, sequential measurements, or spatial data where values change gradually.\n",
    "\n",
    "**Key limitation:** Requires naturally ordered data and fails with large gaps or abrupt changes.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Predictive Model Imputation**\n",
    "\n",
    "Predictive imputation uses machine learning models (regression, KNN, random forests) to estimate missing values based on relationships with other variables.\n",
    "\n",
    "**How it works:** Trains a model on complete cases, then predicts missing values using other variables as features.\n",
    "\n",
    "**When to use:** Strong correlations exist between variables and you have sufficient complete cases for training (>50% complete).\n",
    "\n",
    "**Key limitation:** More complex, computationally intensive, and may overfit or underestimate variance. Methods like MICE (Multiple Imputation by Chained Equations) are beyond this course's scope but are commonly used in research settings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704259fe",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we tackled two crucial data science skills:\n",
    "\n",
    "**Missing Data Analysis:** From detection to strategy selection to impact assessment, you now have a systematic approach to one of data science's biggest challenges.\n",
    "\n",
    "**Integration:** Most importantly, you've learned to combine statistical analysis with sophisticated visualization for robust, honest exploration of messy real-world data.\n",
    "\n",
    "**The Key insight:** Great exploratory data analysis isn't just about creating beautiful charts—it's about making informed decisions about data quality, visualization choices, and analytical strategies that lead to trustworthy insights.\n",
    "\n",
    "Your toolkit now includes the technical skills AND the judgment needed for professional-quality data analysis in the real world, where data is never perfect and every choice matters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef873839",
   "metadata": {},
   "source": [
    "## *Bringing It All Together*\n",
    "\n",
    "This workbook was very different from the previous six tutorials. More conceptual. So to make sure that the salient ideas are not getting lost, choose **one** question from below and answer it on Ed Discussion. Make sure you post to the appropriate thread.\n",
    "\n",
    "**Question 1: Strategic Decision-Making**\n",
    "You have a dataset with 1000 rows and 20 columns. After analysis, you find that 5 columns have 60% missing data, and 8 columns have 15% missing data. If you use Strategy 1 (remove all rows with ANY missing values), you retain only 50 rows. If you use Strategy 2 (remove high-missing columns first, then incomplete rows), you retain 400 rows. Explain why Strategy 2 retains so much more data than Strategy 1, and describe a scenario where Strategy 1 might actually be the better choice despite the massive data loss.\n",
    "\n",
    "\n",
    "**Question 2: Understanding Correlation-Aware Redundancy**\n",
    "In the correlation-aware redundancy removal strategy, we only consider removing variables that have BOTH high missingness AND high correlation with another variable. Explain in your own words: Why is it important to check BOTH conditions (high missingness AND high correlation) rather than just removing all highly correlated variables? Use a **real world example** to illustrate your reasoning.\n",
    "\n",
    "\n",
    "**Question 3: Imputation Trade-offs**\n",
    "Simple statistical imputation (using mean/median) is fast and easy to implement, but it has a major drawback: it reduces variance in the data.\n",
    "\n",
    "**(a)** Explain what \"reduces variance\" means in practical terms—what happens to the distribution of the data after mean imputation?\n",
    "\n",
    "**(b)** Why is this a problem for downstream analysis? Give a specific **real world example** of a type of analysis or visualization where reduced variance would lead to misleading conclusions.\n",
    "\n",
    "\n",
    "**Question 4: Group-Based vs. Simple Imputation**\n",
    "Imagine you're analyzing penguin body measurements, and you have missing values for \"bill length.\" You have three species of penguins in your dataset: Adelie, Gentoo, and Chinstrap, and you know that Gentoo penguins have significantly longer bills than the other two species.\n",
    "\n",
    "Explain why group-based imputation (imputing by species) would give you better results than simple mean imputation (using the overall mean for all penguins). What specific problem does group-based imputation avoid? What specific problems does it acase?\n",
    "\n",
    "\n",
    "**Question 5: Missingness Patterns and Bias**\n",
    "Looking at the Hawks dataset, we discovered that certain attributes (like `wingpitfat`, `tarsus`) have very high percentages of missing data, while others (like `species`, `wing`, `weight`) are mostly complete. Propose TWO possible real-world reasons why `wingpitfat` might be missing much more often than `wing` measurements. Think about the data collection process for studying wild birds.\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
